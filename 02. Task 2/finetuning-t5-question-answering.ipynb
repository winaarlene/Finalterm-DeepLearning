{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5a6ee875",
      "metadata": {
        "id": "5a6ee875"
      },
      "source": [
        "# Task 2: Fine-tuning a Sequence-to-Sequence Model\n",
        "## Deep Learning Final Assignment\n",
        "\n",
        "**Objective:** Fine-tune T5-base model on SQuAD dataset for Generative Question Answering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e7934ff",
      "metadata": {
        "id": "9e7934ff"
      },
      "source": [
        "## Setup and Installation\n",
        "\n",
        "First, we'll install all necessary libraries for this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bc6748be",
      "metadata": {
        "id": "bc6748be"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets torch evaluate rouge-score accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a89dad0",
      "metadata": {
        "id": "3a89dad0"
      },
      "source": [
        "## Import Required Libraries\n",
        "\n",
        "Import all necessary libraries for data processing, model training, and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "04b29bb4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04b29bb4",
        "outputId": "dba5e5f2-80c5-4a24-df59-2c1a899b9e11"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "90"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    T5Tokenizer,\n",
        "    T5ForConditionalGeneration,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForSeq2Seq\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ce38f85",
      "metadata": {
        "id": "6ce38f85"
      },
      "source": [
        "\n",
        "# 1. Data Preprocessing\n",
        "\n",
        "In this section, we'll:\n",
        "1. Load the SQuAD dataset from Hugging Face\n",
        "2. Explore the dataset structure\n",
        "3. Preprocess the data by formatting inputs as 'question: [Q] context: [C]'\n",
        "4. Tokenize the data for T5 model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab4c975c",
      "metadata": {
        "id": "ab4c975c"
      },
      "source": [
        "## 1.1 Load the SQuAD Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "fae44392",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fae44392",
        "outputId": "9dc518ca-999b-47a2-fea8-37147a22bd9e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 87,599 | Validation: 10,570\n"
          ]
        }
      ],
      "source": [
        "dataset = load_dataset(\"squad\")\n",
        "print(f\"Train: {len(dataset['train']):,} | Validation: {len(dataset['validation']):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89ca630c",
      "metadata": {
        "id": "89ca630c"
      },
      "source": [
        "## 1.2 Explore the Dataset\n",
        "\n",
        "Let's look at a sample example to understand the data structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "abe9f140",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abe9f140",
        "outputId": "70b07235-13ca-46ed-956a-9023ea117e4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper sta...\n",
            "\n",
            "Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
            "Answer: Saint Bernadette Soubirous\n"
          ]
        }
      ],
      "source": [
        "sample = dataset['train'][0]\n",
        "\n",
        "print(f\"Context: {sample['context'][:200]}...\")\n",
        "print(f\"\\nQuestion: {sample['question']}\")\n",
        "print(f\"Answer: {sample['answers']['text'][0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e7d7f45",
      "metadata": {
        "id": "1e7d7f45"
      },
      "source": [
        "## 1.3 Initialize T5 Tokenizer\n",
        "\n",
        "Load the T5-base tokenizer that will be used to convert text to tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a57f3d5b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a57f3d5b",
        "outputId": "9ec83b18-36f0-4c06-a35d-ea13685b2b01"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizer loaded | Vocab size: 32,000\n"
          ]
        }
      ],
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "print(f\"Tokenizer loaded | Vocab size: {tokenizer.vocab_size:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1760bd52",
      "metadata": {
        "id": "1760bd52"
      },
      "source": [
        "## 1.4 Data Preprocessing Function\n",
        "\n",
        "Create preprocessing function that formats input as: **'question: [Q] context: [C]'**\n",
        "\n",
        "This is the required format for our generative QA task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "028ed180",
      "metadata": {
        "id": "028ed180"
      },
      "outputs": [],
      "source": [
        "MAX_INPUT_LENGTH = 384  \n",
        "MAX_TARGET_LENGTH = 64   \n",
        "\n",
        "def preprocess_function(examples):\n",
        "    \"\"\"Format: 'question: [Q] context: [C]' -> '[Answer]'\"\"\"\n",
        "    inputs = [\n",
        "        f\"question: {question} context: {context}\"\n",
        "        for question, context in zip(examples['question'], examples['context'])\n",
        "    ]\n",
        "    targets = [answers['text'][0] for answers in examples['answers']]\n",
        "\n",
        "    model_inputs = tokenizer(\n",
        "        inputs,\n",
        "        max_length=MAX_INPUT_LENGTH,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    labels = tokenizer(\n",
        "        targets,\n",
        "        max_length=MAX_TARGET_LENGTH,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5189aed",
      "metadata": {
        "id": "e5189aed"
      },
      "source": [
        "## 1.5 Apply Preprocessing to Dataset\n",
        "\n",
        "Apply the preprocessing function to the entire dataset. For faster training, we'll use a subset of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "177906cd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98,
          "referenced_widgets": [
            "c024034d73624c2087f5d9efaf7c0249",
            "1a5c6ca9b90f48348d7ee11d4a185f94",
            "655394aa242e45b6b888b381c0e564b4",
            "bb64c375d064477aaa83f687fa94cc00",
            "b7747e3d352847758ff1c6e770698cb4",
            "6bcb49d7427d4727ac621e8e47c9d4cc",
            "65065cb07fdd49a79fc6b54ebf3de87f",
            "5b5931c699d346d297e1ef93603eeb84",
            "13f7ed15af8d4c488d0e524f8825ff9a",
            "d920043a897e44d09ccd5c6e2f98ddd6",
            "21e25801738d4997bf35794bd00fc07d",
            "5491f5f5e5ab4f418fb263d09838caad",
            "0e020a86748e437ebdc0bb22f174ef18",
            "f6aa6a857afd4facbf67550bd7e5b774",
            "139a623a71174173b565432646ee59f8",
            "5a90deee6e7a407b94b7116e0d3ac994",
            "b54c766e08a549b2b324d8f0f7843bd4",
            "1b0255da45d344cfb517f849093432e5",
            "9e3c2baa8dcf4600bcf642c08dc7b038",
            "d263d8a10a544655839e5b32c9f963cc",
            "a7867ebc01b64756aff02f1b34960447",
            "8fdd613dbf1d440c9b2b404fcc8c21f7"
          ]
        },
        "id": "177906cd",
        "outputId": "d6c435e4-36a7-4e3b-a5f1-d18558abdbfd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c024034d73624c2087f5d9efaf7c0249",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5491f5f5e5ab4f418fb263d09838caad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessed | Train: 1000 | Val: 200\n"
          ]
        }
      ],
      "source": [
        "TRAIN_SAMPLES = 1000  \n",
        "VAL_SAMPLES = 200     \n",
        "\n",
        "train_dataset = dataset['train'].select(range(TRAIN_SAMPLES))\n",
        "val_dataset = dataset['validation'].select(range(VAL_SAMPLES))\n",
        "\n",
        "tokenized_train = train_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=train_dataset.column_names,\n",
        "    batch_size=16  \n",
        ")\n",
        "\n",
        "tokenized_val = val_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=val_dataset.column_names,\n",
        "    batch_size=16\n",
        ")\n",
        "\n",
        "print(f\"Preprocessed | Train: {len(tokenized_train)} | Val: {len(tokenized_val)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab870f26",
      "metadata": {
        "id": "ab870f26"
      },
      "source": [
        "## 1.6 Verify Preprocessing\n",
        "\n",
        "Let's verify that our preprocessing worked correctly by decoding a sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "b51b0eb1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b51b0eb1",
        "outputId": "62106a92-47b9-4a48-8b8f-6f2507faa13d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? context: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue o...\n",
            "\n",
            "Target: Saint Bernadette Soubirous\n"
          ]
        }
      ],
      "source": [
        "sample_idx = 0\n",
        "input_text = tokenizer.decode(tokenized_train[sample_idx]['input_ids'], skip_special_tokens=True)\n",
        "label_text = tokenizer.decode(tokenized_train[sample_idx]['labels'], skip_special_tokens=True)\n",
        "\n",
        "print(f\"Input: {input_text[:200]}...\")\n",
        "print(f\"\\nTarget: {label_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfc6c448",
      "metadata": {
        "id": "cfc6c448"
      },
      "source": [
        "\n",
        "# 2. Model Training\n",
        "\n",
        "In this section, we'll:\n",
        "1. Load the T5-base model\n",
        "2. Configure training arguments\n",
        "3. Set up the Trainer API\n",
        "4. Fine-tune the model on SQuAD dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8928011",
      "metadata": {
        "id": "c8928011"
      },
      "source": [
        "## 2.1 Load T5-base Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1c147e5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1c147e5",
        "outputId": "7cab1efa-689d-4aab-96dc-bea319e37ac6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded | Parameters: 222,903,552\n",
            "Gradient checkpointing enabled for memory optimization\n"
          ]
        }
      ],
      "source": [
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
        "model = model.to(device)\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "print(f\"Model loaded | Parameters: {model.num_parameters():,}\")\n",
        "print(\"Gradient checkpointing enabled for memory optimization\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61dbc7af",
      "metadata": {
        "id": "61dbc7af"
      },
      "source": [
        "## 2.2 Setup Data Collator\n",
        "\n",
        "Data collator handles dynamic padding and prepares batches for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "56fa270a",
      "metadata": {
        "id": "56fa270a"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    padding=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f1421f2",
      "metadata": {
        "id": "2f1421f2"
      },
      "source": [
        "## 2.3 Configure Training Arguments\n",
        "\n",
        "Set up hyperparameters and training configuration using Hugging Face TrainingArguments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "5370b7d9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5370b7d9",
        "outputId": "92e589b3-476f-49ba-9971-7392ce31b079"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training: 2 epochs | Batch: 4 | Grad Accum: 4\n",
            "Effective batch size: 16\n"
          ]
        }
      ],
      "source": [
        "OUTPUT_DIR = \"./t5-squad-finetuned\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=3e-4,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=1,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    warmup_steps=100,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    dataloader_num_workers=0,\n",
        "    report_to=\"none\",\n",
        "    push_to_hub=False,\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"adafactor\",\n",
        ")\n",
        "\n",
        "print(f\"Training: {training_args.num_train_epochs} epochs | Batch: {training_args.per_device_train_batch_size} | Grad Accum: {training_args.gradient_accumulation_steps}\")\n",
        "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5fb6bae",
      "metadata": {
        "id": "f5fb6bae"
      },
      "source": [
        "## 2.4 Initialize Trainer\n",
        "\n",
        "Create a Trainer instance with our model, datasets, and training configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "3647bb50",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3647bb50",
        "outputId": "3a0ed4d0-c064-42e3-ac04-1793e22fc706"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1846427416.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4f93e5c",
      "metadata": {
        "id": "e4f93e5c"
      },
      "source": [
        "## 2.5 Start Training\n",
        "\n",
        "Begin the fine-tuning process. This may take some time depending on your hardware.\n",
        "\n",
        "**Note:** Training 5000 samples for 3 epochs will take approximately:\n",
        "- With GPU: 30-60 minutes\n",
        "- With CPU: 4-6 hours"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "212796b4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "212796b4",
        "outputId": "6ac49fbb-b06e-4b73-fae3-48253efd5ef3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training started...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [126/126 3:38:42, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>7.663700</td>\n",
              "      <td>0.024563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.026800</td>\n",
              "      <td>0.022383</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "Training Loss: 3.0566\n",
            "Runtime: 13239.30s\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"Training started...\\n\")\n",
        "\n",
        "train_result = trainer.train()\n",
        "\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"Training Loss: {train_result.training_loss:.4f}\")\n",
        "print(f\"Runtime: {train_result.metrics['train_runtime']:.2f}s\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cd22137",
      "metadata": {
        "id": "9cd22137"
      },
      "source": [
        "## 2.6 Save the Fine-tuned Model\n",
        "\n",
        "Save the model and tokenizer for future use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "c9fae289",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9fae289",
        "outputId": "4b743417-9d9b-40a8-fca7-266be03bd0bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to: ./t5-squad-final\n"
          ]
        }
      ],
      "source": [
        "FINAL_MODEL_DIR = \"./t5-squad-final\"\n",
        "\n",
        "trainer.save_model(FINAL_MODEL_DIR)\n",
        "tokenizer.save_pretrained(FINAL_MODEL_DIR)\n",
        "\n",
        "print(f\"Model saved to: {FINAL_MODEL_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "160f7caa",
      "metadata": {
        "id": "160f7caa"
      },
      "source": [
        "\n",
        "# 3. Model Evaluation\n",
        "\n",
        "In this section, we'll:\n",
        "1. Evaluate the model on the validation set\n",
        "2. Test the model with custom questions and contexts\n",
        "3. Demonstrate the model's question-answering capabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88096e70",
      "metadata": {
        "id": "88096e70"
      },
      "source": [
        "## 3.1 Evaluate on Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "2b765261",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "2b765261",
        "outputId": "c5b9240d-a8ca-423a-e6ec-abe43c5bb62a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 05:01]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Validation Results:\n",
            "================================================================================\n",
            "Loss: 0.0224\n",
            "Runtime: 306.58s\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "eval_results = trainer.evaluate()\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"Validation Results:\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Loss: {eval_results['eval_loss']:.4f}\")\n",
        "print(f\"Runtime: {eval_results['eval_runtime']:.2f}s\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7bceae3",
      "metadata": {
        "id": "b7bceae3"
      },
      "source": [
        "## 3.2 Create Inference Function\n",
        "\n",
        "Create a helper function to generate answers for custom questions and contexts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "ec5e52b4",
      "metadata": {
        "id": "ec5e52b4"
      },
      "outputs": [],
      "source": [
        "def generate_answer(question, context, max_length=128):\n",
        "    \"\"\"Generate answer from question and context.\"\"\"\n",
        "    input_text = f\"question: {question} context: {context}\"\n",
        "\n",
        "    input_ids = tokenizer(\n",
        "        input_text,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=512,\n",
        "        truncation=True\n",
        "    ).input_ids.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids,\n",
        "            max_length=max_length,\n",
        "            num_beams=4,\n",
        "            early_stopping=True,\n",
        "            temperature=0.7,\n",
        "        )\n",
        "\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58fddffa",
      "metadata": {
        "id": "58fddffa"
      },
      "source": [
        "## 3.3 Test with Examples from Validation Set\n",
        "\n",
        "Let's test the model with examples from the SQuAD validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "676e8d01",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "676e8d01",
        "outputId": "b7df3804-1d1c-4465-8aee-29e2684de198"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Context: For the third straight season, the number one seeds from both conferences met in the Super Bowl. The Carolina Panthers became one of only ten teams to have completed a regular season with only one los...\n",
            "\n",
            "Question: What seed was the Denver Broncos?\n",
            "True Answer: number one\n",
            "Predicted: number one\n",
            "\n",
            "Example 2:\n",
            "Context: The Broncos took an early lead in Super Bowl 50 and never trailed. Newton was limited by Denver's defense, which sacked him seven times and forced him into three turnovers, including a fumble which th...\n",
            "\n",
            "Question: How many tackles did Von Miller get during the game?\n",
            "True Answer: 5\n",
            "Predicted: five\n",
            "\n",
            "Example 3:\n",
            "Context: The Broncos took an early lead in Super Bowl 50 and never trailed. Newton was limited by Denver's defense, which sacked him seven times and forced him into three turnovers, including a fumble which th...\n",
            "\n",
            "Question: Who won the Super Bowl MVP?\n",
            "True Answer: Von Miller\n",
            "Predicted: Von Miller\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "test_indices = random.sample(range(len(val_dataset)), 3)\n",
        "\n",
        "for i, idx in enumerate(test_indices, 1):\n",
        "    example = val_dataset[idx]\n",
        "    predicted = generate_answer(example['question'], example['context'])\n",
        "\n",
        "    print(f\"\\nExample {i}:\")\n",
        "    print(f\"Context: {example['context'][:200]}...\")\n",
        "    print(f\"\\nQuestion: {example['question']}\")\n",
        "    print(f\"True Answer: {example['answers']['text'][0]}\")\n",
        "    print(f\"Predicted: {predicted}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc69d1c8",
      "metadata": {
        "id": "bc69d1c8"
      },
      "source": [
        "## 3.4 Test with Custom Question and Context\n",
        "\n",
        "Now let's test the model with a completely custom example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "980cb22b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "980cb22b",
        "outputId": "ad7e65fc-433b-4a77-b103-684a35b90c55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: When was the Transformer model introduced?\n",
            "Answer: 2017\n"
          ]
        }
      ],
      "source": [
        "custom_context = \"\"\"\n",
        "The Transformer is a deep learning model introduced in 2017, used primarily in the field of\n",
        "natural language processing (NLP). It was proposed in the paper 'Attention is All You Need' by\n",
        "Vaswani et al. The Transformer architecture uses self-attention mechanisms to process input\n",
        "sequences in parallel, making it more efficient than recurrent neural networks (RNNs).\n",
        "This architecture has become the foundation for many state-of-the-art models like BERT, GPT,\n",
        "and T5.\n",
        "\"\"\"\n",
        "\n",
        "custom_question = \"When was the Transformer model introduced?\"\n",
        "custom_answer = generate_answer(custom_question, custom_context)\n",
        "\n",
        "print(f\"Question: {custom_question}\")\n",
        "print(f\"Answer: {custom_answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9e7e69c",
      "metadata": {
        "id": "f9e7e69c"
      },
      "source": [
        "## 3.5 Interactive Testing\n",
        "\n",
        "Test the model with your own questions and contexts!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "ff834e46",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff834e46",
        "outputId": "abf29ae5-f713-4a16-8cb5-98a5800acf0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "1. Q: What is Deep Learning?\n",
            "   A: a subset of machine learning\n",
            "\n",
            "2. Q: Who created Python?\n",
            "   A: Guido van Rossum\n",
            "\n",
            "3. Q: What dataset was T5 pre-trained on?\n",
            "   A: Colossal Clean Crawled Corpus\n"
          ]
        }
      ],
      "source": [
        "test_examples = [\n",
        "    {\n",
        "        \"context\": \"\"\"Deep Learning is a subset of machine learning that uses artificial neural\n",
        "        networks with multiple layers to learn from data. It has achieved remarkable success in\n",
        "        various domains including computer vision, natural language processing, and speech recognition.\"\"\",\n",
        "        \"question\": \"What is Deep Learning?\"\n",
        "    },\n",
        "    {\n",
        "        \"context\": \"\"\"Python is a high-level, interpreted programming language created by Guido\n",
        "        van Rossum and first released in 1991. Python emphasizes code readability with its notable\n",
        "        use of significant whitespace.\"\"\",\n",
        "        \"question\": \"Who created Python?\"\n",
        "    },\n",
        "    {\n",
        "        \"context\": \"\"\"The T5 (Text-to-Text Transfer Transformer) model treats every NLP problem\n",
        "        as a text-to-text problem. T5 was pre-trained on the Colossal Clean Crawled Corpus (C4)\n",
        "        dataset and can be fine-tuned for specific tasks.\"\"\",\n",
        "        \"question\": \"What dataset was T5 pre-trained on?\"\n",
        "    }\n",
        "]\n",
        "\n",
        "for i, example in enumerate(test_examples, 1):\n",
        "    answer = generate_answer(example[\"question\"], example[\"context\"])\n",
        "    print(f\"\\n{i}. Q: {example['question']}\")\n",
        "    print(f\"   A: {answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41bb2b39",
      "metadata": {
        "id": "41bb2b39"
      },
      "source": [
        "## 3.6 Model Performance Summary\n",
        "\n",
        "Let's create a summary of our model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "c916eebe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c916eebe",
        "outputId": "aff5bcf2-8895-4978-a689-89e8f485eef8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PERFORMANCE SUMMARY\n",
            "Model: T5-base fine-tuned on SQuAD\n",
            "Training Samples: 1,000\n",
            "Validation Samples: 200\n",
            "Epochs: 2\n",
            "\n",
            "Training Loss: 3.0566\n",
            "Validation Loss: 0.0224\n",
            "\n",
            "Saved: ./t5-squad-final\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"PERFORMANCE SUMMARY\")\n",
        "\n",
        "print(f\"Model: T5-base fine-tuned on SQuAD\")\n",
        "print(f\"Training Samples: {len(tokenized_train):,}\")\n",
        "print(f\"Validation Samples: {len(tokenized_val):,}\")\n",
        "print(f\"Epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"\\nTraining Loss: {train_result.training_loss:.4f}\")\n",
        "print(f\"Validation Loss: {eval_results['eval_loss']:.4f}\")\n",
        "print(f\"\\nSaved: {FINAL_MODEL_DIR}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0e020a86748e437ebdc0bb22f174ef18": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b54c766e08a549b2b324d8f0f7843bd4",
            "placeholder": "​",
            "style": "IPY_MODEL_1b0255da45d344cfb517f849093432e5",
            "value": "Map: 100%"
          }
        },
        "139a623a71174173b565432646ee59f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7867ebc01b64756aff02f1b34960447",
            "placeholder": "​",
            "style": "IPY_MODEL_8fdd613dbf1d440c9b2b404fcc8c21f7",
            "value": " 200/200 [00:00&lt;00:00, 792.74 examples/s]"
          }
        },
        "13f7ed15af8d4c488d0e524f8825ff9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1a5c6ca9b90f48348d7ee11d4a185f94": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6bcb49d7427d4727ac621e8e47c9d4cc",
            "placeholder": "​",
            "style": "IPY_MODEL_65065cb07fdd49a79fc6b54ebf3de87f",
            "value": "Map: 100%"
          }
        },
        "1b0255da45d344cfb517f849093432e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21e25801738d4997bf35794bd00fc07d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5491f5f5e5ab4f418fb263d09838caad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0e020a86748e437ebdc0bb22f174ef18",
              "IPY_MODEL_f6aa6a857afd4facbf67550bd7e5b774",
              "IPY_MODEL_139a623a71174173b565432646ee59f8"
            ],
            "layout": "IPY_MODEL_5a90deee6e7a407b94b7116e0d3ac994"
          }
        },
        "5a90deee6e7a407b94b7116e0d3ac994": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b5931c699d346d297e1ef93603eeb84": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65065cb07fdd49a79fc6b54ebf3de87f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "655394aa242e45b6b888b381c0e564b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b5931c699d346d297e1ef93603eeb84",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_13f7ed15af8d4c488d0e524f8825ff9a",
            "value": 1000
          }
        },
        "6bcb49d7427d4727ac621e8e47c9d4cc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fdd613dbf1d440c9b2b404fcc8c21f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e3c2baa8dcf4600bcf642c08dc7b038": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7867ebc01b64756aff02f1b34960447": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b54c766e08a549b2b324d8f0f7843bd4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7747e3d352847758ff1c6e770698cb4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb64c375d064477aaa83f687fa94cc00": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d920043a897e44d09ccd5c6e2f98ddd6",
            "placeholder": "​",
            "style": "IPY_MODEL_21e25801738d4997bf35794bd00fc07d",
            "value": " 1000/1000 [00:02&lt;00:00, 521.58 examples/s]"
          }
        },
        "c024034d73624c2087f5d9efaf7c0249": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1a5c6ca9b90f48348d7ee11d4a185f94",
              "IPY_MODEL_655394aa242e45b6b888b381c0e564b4",
              "IPY_MODEL_bb64c375d064477aaa83f687fa94cc00"
            ],
            "layout": "IPY_MODEL_b7747e3d352847758ff1c6e770698cb4"
          }
        },
        "d263d8a10a544655839e5b32c9f963cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d920043a897e44d09ccd5c6e2f98ddd6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6aa6a857afd4facbf67550bd7e5b774": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e3c2baa8dcf4600bcf642c08dc7b038",
            "max": 200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d263d8a10a544655839e5b32c9f963cc",
            "value": 200
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
